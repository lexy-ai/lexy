{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image search tutorial\n",
    "\n",
    "In this tutorial, we'll walk through how to use Lexy to create a multimodal search application. We'll use the [CLIP](https://openai.com/blog/clip/) model from OpenAI to create embeddings for images, and then use those embeddings to find matching images for a given text query, or vice versa."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d941a6ba4292f46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lexy_py import LexyClient\n",
    "\n",
    "lexy = LexyClient()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d554b748f2e859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create collection\n",
    "\n",
    "Let's first create a collection to store our images. We'll use the `images_tutorial` collection for this tutorial."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fb3eab8a6eef5ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new collection\n",
    "images_tutorial = lexy.create_collection('images_tutorial')\n",
    "images_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create index and binding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bad6726cef20a9d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define index\n",
    "\n",
    "First we'll define our index to store our embedded images. We use `*.embeddings.clip` as the transformer model name to indicate that we want to use the CLIP embeddings model, but that the embedding field can use any model that matches this pattern, including `image.embeddings.clip` and `text.embeddings.clip`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e99d50be865c721"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define index fields\n",
    "index_fields = {\n",
    "    \"embedding\": {\"type\": \"embedding\", \"extras\": {\"dims\": 512, \"model\": \"*.embeddings.clip\"}},\n",
    "}\n",
    "\n",
    "# create index\n",
    "idx = lexy.create_index(\n",
    "    index_id='image_tutorial_index', \n",
    "    description='Index for images tutorial',\n",
    "    index_fields=index_fields\n",
    ")\n",
    "idx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42f382c4a41abfc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll use the CLIP image embeddings transformer available on [HuggingFace](https://huggingface.co/openai/clip-vit-base-patch32). This transformer uses the [CLIP](https://openai.com/blog/clip/) model from OpenAI to create embeddings for images. \n",
    "\n",
    "The CLIP model is a transformer model that was trained on a large dataset of images and text pairs. The model learns to map images and text to a shared embedding space, where the embeddings of matching images and text are close together. We can use this model to create embeddings for images, and then use those embeddings to find matching images for a given text query, or vice versa."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f78702c8284a7451"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lexy.transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa825ae756532889"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create binding\n",
    "\n",
    "We'll create a binding that will process images added to our `images_tutorial` collection using the CLIP image embeddings transformer, and store the results in `image_tutorial_index`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1118f4503cf794f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "binding = lexy.create_binding(collection_id='images_tutorial',\n",
    "                              transformer_id='image.embeddings.clip',\n",
    "                              index_id='image_tutorial_index')\n",
    "binding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51ae8aedd267534"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload images to the collection\n",
    "\n",
    "Let's upload some images from the [image-text-demo dataset](https://huggingface.co/datasets/jamescalam/image-text-demo) to the collection. This dataset is from HuggingFace datasets and requires the `datasets` package to be installed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae090c5f38acf0a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aad7cb41f11dd194"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import test data from HuggingFace datasets - requires `pip install datasets`\n",
    "\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"jamescalam/image-text-demo\", split=\"train\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0f4a122d529eb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd9e489fed11159"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add documents to the collection\n",
    "for i, row in enumerate(data, start=1):\n",
    "    print(i, row['text'])\n",
    "    lexy.upload_documents(files=row['image'], \n",
    "                          filenames=row['text'] + '.jpg', \n",
    "                          collection_id='images_tutorial')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85d0786564221623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the collection\n",
    "images_tutorial.list_documents()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de033fac19a79621"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query index\n",
    "\n",
    "Let's first define some helper functions to display our image results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3b341b2233ef1ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import httpx\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "def image_from_url(url): \n",
    "    response = httpx.get(url)\n",
    "    response.raise_for_status()\n",
    "    return Image.open(response)\n",
    "\n",
    "def display_results_html(records):\n",
    "    html_content = \"\"\n",
    "    for r in records:\n",
    "        d = r['document']\n",
    "        thumbnail_url = d.thumbnail_url\n",
    "        fname = d.meta.get('filename')\n",
    "        score = f\"score: {r['distance']:.4f}\"\n",
    "        # Creating a row for each result with image on the left and text on the right\n",
    "        html_content += f\"\"\"\n",
    "        <div style='display: flex; align-items: center; margin-bottom: 20px; margin-top: 20px;'>\n",
    "            <img src='{thumbnail_url}' style='width: auto; height: auto; margin-right: 20px;'/>\n",
    "            <div>\n",
    "                <p>{fname}</p>\n",
    "                <p>{score}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    # Display all results as HTML\n",
    "    display(HTML(html_content))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "766431848007ae4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query by text\n",
    "\n",
    "We can query our index by text to find matching images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f176e09dbe6012dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = idx.query(query_text='best friends', return_document=True)\n",
    "display_results_html(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7f98e2ec27a4d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = idx.query(query_text='gotham city', return_document=True)\n",
    "display_results_html(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "379807d4dec74330"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query by image\n",
    "\n",
    "We can also query our index by image to find matching images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d131788a58cfb82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = image_from_url('https://getlexy.com/assets/images/dalle-agi.jpeg')\n",
    "img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78a4adffb1613b7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = idx.query(query_image=img, return_document=True)\n",
    "display_results_html(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "804c783d94d749c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = image_from_url('https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Night_in_the_Greater_Tokyo_Area_ISS054.jpg/2560px-Night_in_the_Greater_Tokyo_Area_ISS054.jpg')\n",
    "img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c8279bf568e9a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = idx.query(query_image=img, return_document=True)\n",
    "display_results_html(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5090720d9f499969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = image_from_url('https://upload.wikimedia.org/wikipedia/commons/e/ed/Shanghai_skyline_2018%28cropped%29.jpg')\n",
    "img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5466dea33e794ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = idx.query(query_image=img, return_document=True)\n",
    "display_results_html(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10ef7c45f262a71c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6013f4c54da4bb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lexy.delete_binding(binding_id=binding.id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "974fa2bb441aff87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lexy.delete_index('image_tutorial_index', drop_table=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b105016f9cb27117"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lexy.delete_collection('images_tutorial', delete_documents=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357001a4619d1179"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5d5fd0146374414d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
