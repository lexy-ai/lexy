{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Example: Building code search with Lexy\n",
    "\n",
    "In this example, we'll create a semantic search engine that allows us to run similarity search using **only the comments and docstrings** in code from a given GitHub repository. Below is an overview of the steps involved.\n",
    "\n",
    "**Part 1: Ingesting raw code from GitHub**. Right now, this step requires LlamaIndex (but won't in the future).\n",
    "\n",
    "**Part 2: Extracting comments and docstrings**. We'll write a function to parse comments and docstrings from code files. We'll use Lexy to run our function on our documents, and to write the output to an index for querying.\n",
    "\n",
    "**Part 3: Running similarity search queries**. We'll use Lexy to run similarity search queries against our newly created index of comments and docstrings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30991ac5dc8c8f30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: Add a diagram here showing an example file, e.g., `main.py`, split into multiple rows of comments and docstrings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c6a14774f38eea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Ingesting data\n",
    "\n",
    "For now, we're going to clone our repo locally and import the files using `llama_index.SimpleDirectoryReader`. This step requires **LlamaIndex**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b95dc41f0c9e30b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: Once the `Collection` model supports custom PKs, include an explanation of how to choose your PK based on how you want your application and update logic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48de46a47e9ce6aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if llama-index is installed\n",
    "! pip freeze | grep -i llama-index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "973d8bfd452f8ce9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# install llama-index if not installed\n",
    "! pip install llama-index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f53d7345e9c331"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from llama_index import SimpleDirectoryReader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c24f804a0a55a374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use this for the event loop to work in Jupyter notebooks\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c14036c8cfe5aa88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clone a repo locally"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efd8e38e68432bf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is the relative path to the directory where we git clone our repos\n",
    "local_repo_dir = \"../tmp\"\n",
    "\n",
    "# create the temporary directory if it doesn't exist\n",
    "os.makedirs(local_repo_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def clone_repo(repo_url, repo_path_prefix=local_repo_dir, n_top_ext=30):\n",
    "    \"\"\" Clone a GitHub repo and print some stats about it. \"\"\"\n",
    "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
    "    repo_path = os.path.join(repo_path_prefix, repo_name)\n",
    "    \n",
    "    # clone the repo if it doesn't exist\n",
    "    if os.path.exists(repo_path):\n",
    "        print(f\"Repo '{repo_url}' already exists at {repo_path}\")\n",
    "    else:\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", repo_url], cwd=repo_path_prefix)\n",
    "        print(f\"Repo '{repo_url}' cloned to {repo_path}\")\n",
    "\n",
    "    # count total files\n",
    "    n_total_files = subprocess.check_output([\"git\", \"ls-files\", \"-z\"], cwd=repo_path).decode(\"utf-8\").count(\"\\0\")\n",
    "    print(f\"Total files in {repo_name}: {int(n_total_files)}\")\n",
    "    \n",
    "    # get all file extensions and count them\n",
    "    files = subprocess.check_output([\"git\", \"ls-files\"], cwd=repo_path).decode(\"utf-8\").splitlines()\n",
    "    ext_counts = {}\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1]\n",
    "        if ext:\n",
    "            ext_counts[ext] = ext_counts.get(ext, 0) + 1\n",
    "    \n",
    "    # sort extensions by count and print top n_top_ext\n",
    "    sorted_ext_counts = sorted(ext_counts.items(), key=lambda item: item[1], reverse=True)[:n_top_ext]\n",
    "    print(f\"Top {n_top_ext} extensions:\")\n",
    "    for ext, count in sorted_ext_counts:\n",
    "        print(f\"\\t{ext}: {count}\")\n",
    "    \n",
    "    return repo_name, repo_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c23447187f207d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "repo_name, repo_path = clone_repo(\"https://github.com/ray-project/ray.git\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d68246f99fd198f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# specify the file extensions we want to include when ingesting the repo\n",
    "include_extensions = [\n",
    "    \".py\", \n",
    "    # \".ipynb\",  # errors out in llama_index, skipping for now\n",
    "    \".java\", \n",
    "    \".c\", \".h\", \n",
    "    \".cpp\", \".cc\", \".hpp\", \n",
    "    \".go\", \n",
    "    \".rs\", \n",
    "    \".rb\", \".erb\",\n",
    "    \".js\", \".jsx\", \n",
    "    \".ts\", \".tsx\", \n",
    "    \".html\", \n",
    "    \".css\", \n",
    "    \".sh\",\n",
    "    # \".md\", \".rst\",  # llama_index splits these into nodes, skipping for now\n",
    "    # \".txt\",  # no comments or docstrings in text files\n",
    "    # \".jpg\"  # just testing\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaeb8cd480f8a44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(repo_path, \n",
    "                               filename_as_id=True,\n",
    "                               recursive=True,\n",
    "                               required_exts=include_extensions)\n",
    "llama_docs = reader.load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be2a79dbc1b29117"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert a llama index doc to a lexy doc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "523bf6d7c6ab5dcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lexy_py import Document, LexyClient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6453111f3c8675b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def llama_to_lexy(llama_doc, repo_path_prefix=local_repo_dir) -> Document:\n",
    "    \"\"\" Convert a llama index document to a lexy document \"\"\"\n",
    "    lexy_doc = Document(content=llama_doc.get_text(), meta=llama_doc.dict().get(\"metadata\", {}))\n",
    "    # remove the file path prefix\n",
    "    if \"file_path\" in lexy_doc.meta:\n",
    "        lexy_doc.meta[\"file_path\"] = os.path.relpath(lexy_doc.meta.get(\"file_path\"), repo_path_prefix)\n",
    "    # add file extension\n",
    "    if \"file_name\" in lexy_doc.meta:\n",
    "        _, file_ext = os.path.splitext(lexy_doc.meta[\"file_name\"])\n",
    "        lexy_doc.meta[\"file_ext\"] = file_ext\n",
    "    # add repo name? naw, that's a user space thing, separate from converting to lexy document\n",
    "    # TODO: if an image, upload to lexy as image document\n",
    "    return lexy_doc\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a50c11d8640271c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llama_doc = llama_docs[0]\n",
    "llama_doc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d21b85388425d08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lexy_doc = llama_to_lexy(llama_doc)\n",
    "lexy_doc.dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a1f7ef416f53740"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert all llama docs to lexy docs\n",
    "lexy_docs = [llama_to_lexy(doc) for doc in llama_docs]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcae082ef7d462fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upload docs to Lexy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66fe900cce700ea6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiate Lexy client\n",
    "lx = LexyClient()\n",
    "lx.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31e995d4a189fd5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a collection for our new documents\n",
    "github_repos_collection = lx.create_collection(\"github_repos\", description=\"Code from select Github repositories\")\n",
    "github_repos_collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d806135d62e0a4a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(lexy_docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83ec0a9852cabaf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the lexy docs to our new collection \n",
    "docs_added = lx.add_documents(lexy_docs, collection_id=\"github_repos\")\n",
    "docs_added[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adee84d8e0d5cda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline for ingesting a new repo\n",
    "\n",
    "Now that we have the code to ingest a new repo, we can wrap it in a function and use it to streamline the ingestion of any new repo we want to add."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70a6798aed77365"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lexy_docs_from_github_repo(repo_url: str, \n",
    "                               repo_path_prefix: str = local_repo_dir, \n",
    "                               file_extensions: list[str] = include_extensions) -> list[Document]:\n",
    "    \"\"\" Clones a GitHub repo and returns a list of documents ready for upload to Lexy. \"\"\"\n",
    "    # clone the repo locally\n",
    "    name, path = clone_repo(repo_url, repo_path_prefix)\n",
    "    # read using llama_index\n",
    "    llama_reader = SimpleDirectoryReader(path, \n",
    "                                         filename_as_id=True,\n",
    "                                         recursive=True,\n",
    "                                         required_exts=file_extensions)\n",
    "    llama_repo_docs = llama_reader.load_data()\n",
    "    # convert to lexy docs\n",
    "    lexy_repo_docs = [llama_to_lexy(doc) for doc in llama_repo_docs]\n",
    "    return lexy_repo_docs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed83f78de65d16c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get docs for a new repo\n",
    "repo_docs = lexy_docs_from_github_repo(\"https://github.com/mosaicml/composer.git\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba843e10ff3acbcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(repo_docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9db18c9cf333de7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use this to filter out any known bad files\n",
    "exclude_filenames = [\n",
    "    # \"broken.js\",  # this file contains an invalid null byte and is used for testing\n",
    "]\n",
    "\n",
    "docs_to_add = [d for d in repo_docs if d.meta.get(\"file_name\") not in exclude_filenames]\n",
    "len(docs_to_add)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67e7df4ad95e554b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# upload to lexy\n",
    "docs_added = lx.add_documents(docs_to_add, collection_id=\"github_repos\")\n",
    "docs_added[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97109739f78822ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Extracting comments and docstrings\n",
    "\n",
    "In this part, we'll write a function to parse comments and docstrings from code files. We'll use Lexy to run our function on our documents, and to write the output to an index for querying.\n",
    "\n",
    "Much of this section is included in the tutorial on [custom transformers](https://getlexy.com/tutorials/custom-transformers/)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46d013e37df19179"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using `tree-sitter-languages`, we come up with the following code to extract comments and docstrings from code for a variety of languages (C++, Python, Typescript, and TSX)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7edc44b38546039"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tree_sitter_languages\n",
    "\n",
    "from lexy.models.document import Document\n",
    "from lexy.transformers import lexy_transformer\n",
    "from lexy.transformers.embeddings import text_embeddings\n",
    "\n",
    "\n",
    "lang_from_ext = {\n",
    "    'cc': 'cpp',\n",
    "    'h': 'cpp',\n",
    "    'py': 'python',\n",
    "    'ts': 'typescript',\n",
    "    'tsx': 'tsx',\n",
    "}\n",
    "\n",
    "COMMENT_PATTERN_CPP = \"(comment) @comment\"\n",
    "COMMENT_PATTERN_PY = \"\"\"\n",
    "    (module . (comment)* . (expression_statement (string)) @module_doc_str)\n",
    "\n",
    "    (class_definition\n",
    "        body: (block . (expression_statement (string)) @class_doc_str))\n",
    "\n",
    "    (function_definition\n",
    "        body: (block . (expression_statement (string)) @function_doc_str))\n",
    "\"\"\"\n",
    "COMMENT_PATTERN_TS = \"(comment) @comment\"\n",
    "COMMENT_PATTERN_TSX = \"(comment) @comment\"\n",
    "\n",
    "comment_patterns = {\n",
    "    'cpp': COMMENT_PATTERN_CPP,\n",
    "    'python': COMMENT_PATTERN_PY,\n",
    "    'typescript': COMMENT_PATTERN_TS,\n",
    "    'tsx': COMMENT_PATTERN_TSX\n",
    "}\n",
    "\n",
    "\n",
    "@lexy_transformer(name='code.extract_comments.v1')\n",
    "def get_comments(doc: Document) -> list[dict]:\n",
    "    lang = lang_from_ext.get(doc.meta['file_ext'].replace('.', ''))\n",
    "    comment_pattern = comment_patterns.get(lang, None)\n",
    "\n",
    "    if comment_pattern is None:\n",
    "        return []\n",
    "\n",
    "    parser = tree_sitter_languages.get_parser(lang)\n",
    "    language = tree_sitter_languages.get_language(lang)\n",
    "\n",
    "    tree = parser.parse(bytes(doc.content, \"utf-8\"))\n",
    "    root = tree.root_node\n",
    "\n",
    "    query = language.query(comment_pattern)\n",
    "    matches = query.captures(root)\n",
    "    comments = []\n",
    "    for m, name in matches:\n",
    "        comment_text = m.text.decode('utf-8')\n",
    "        c = {\n",
    "            'comment_text': comment_text,\n",
    "            'comment_embedding': text_embeddings(comment_text),\n",
    "            'comment_meta': {\n",
    "                'start_point': m.start_point,\n",
    "                'end_point': m.end_point,\n",
    "                'type': name\n",
    "            }\n",
    "        }\n",
    "        comments.append(c)\n",
    "    return comments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2ed06d214fbf755"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test on sample documents\n",
    "\n",
    "Let's test our function on a few documents to see if it's working as expected. You'll have to replace the `document_id` with the `document_id` of a document in your collection."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f3a1128e5193c6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# typescript\n",
    "\"\"\"\n",
    "    select document_id from documents d \n",
    "    where collection_id = 'github_repos' \n",
    "    and meta->>'file_path' = 'turbo/packages/turbo-gen/src/commands/raw/index.ts';\n",
    "\"\"\"\n",
    "ts_doc_id = 'f799cabc-2a14-464f-af1d-a1848ae8bd40'\n",
    "ts_doc = lx.get_document(ts_doc_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40887ab90277e0c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_doc.dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd81ab1bce8e4af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = get_comments(ts_doc)\n",
    "print(*[{k: v for k, v in d.items() if k != 'comment_embedding'} for d in c], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caa717d279ea7a75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cpp\n",
    "\"\"\"\n",
    "    select document_id from documents d \n",
    "    where collection_id = 'github_repos' \n",
    "    and meta->>'file_path' = 'ray/cpp/include/ray/api/metric.h';\n",
    "\"\"\"\n",
    "cpp_doc_id = '9e9f73de-dbfb-4a2a-ac66-093b348685fb'\n",
    "cpp_doc = lx.get_document(cpp_doc_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eec837749d5b608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cpp_doc.dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afc684cfd9b59986"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = get_comments(cpp_doc)\n",
    "print(*[{k: v for k, v in d.items() if k != 'comment_embedding'} for d in c], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "653b236da8b9bd71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "\"\"\"\n",
    "    select document_id from documents d \n",
    "    where collection_id = 'github_repos' \n",
    "    and meta->>'file_path' = 'composer/composer/algorithms/alibi/alibi.py';\n",
    "\"\"\"\n",
    "py_doc_id = '30bbb9e1-2f9d-484a-b0d3-409a65fcfdd5'\n",
    "py_doc = lx.get_document(py_doc_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7750a1779e4887a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "py_doc.dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23575d98b7210476"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = get_comments(py_doc)\n",
    "print(*[{k: v for k, v in d.items() if k != 'comment_embedding'} for d in c], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c8f1546383caba3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Registering the function with Lexy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3875fb3ff6690f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like our function is working as expected. To run it against all of our documents, we can follow the instructions in the custom transformers tutorial. We'll use the `lexy_transformer` decorator to register our function with Lexy, and then use the `LexyClient` to run our function on our documents and write the output to an index."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "845dbb24c5fb5361"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We put the above code into a file called `code.py` and place it inside of the `lexy.transformers` directory. \n",
    "\n",
    "*Additional instructions...*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8d4cb069cf40506"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: the things you get from Lexy include the following..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e061b3ec1ab90aef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Running similarity search queries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e714bb520f790692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f35b72537561896"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
