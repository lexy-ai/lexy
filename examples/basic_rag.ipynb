{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa97b77b8d49bf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Basic RAG using Lexy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb913df1d83e31f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Let's go through a basic implementation of Retrieval Augmented Generation (RAG) using Lexy. RAG is the process of using a retriever to find relevant documents to include in the prompt as context for a language model. \n",
    "\n",
    "In this example, we'll use Lexy to store and retrieve documents describing characters from the TV show House of the Dragon. We'll then use those documents to construct a prompt that GPT-4 can use to answer questions about the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc378d4f02b936b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "Note that this example requires an OpenAI API key. You can add your API key as an environment variable using the `.env` file in the root directory of the Lexy repository. See [How do I add a new environment variable](../faq.md#how-do-i-add-a-new-environment-variable) on the [FAQ page](../faq.md) for more details.\n",
    "\n",
    "```shell title=\".env\"\n",
    "OPENAI_API_KEY=your_secret_api_key\n",
    "```\n",
    "\n",
    "Remember to rebuild your containers after adding the environment variable. Simply run the following on the command line.\n",
    "\n",
    "```shell\n",
    "make update-dev-containers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cccb17a48b371e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sample data\n",
    "\n",
    "Our data is in the `sample_data/documents` directory. Let's import it and take a look at the first few lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164d5f48c2994b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../sample_data/documents/hotd.txt\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "lines[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a384ceeb2344798",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add documents to Lexy\n",
    "\n",
    "Now let's instantiate a Lexy client and create a new collection for our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7961d09f28b4434",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lexy_py import LexyClient\n",
    "\n",
    "lexy = LexyClient()\n",
    "\n",
    "# create a new collection\n",
    "collection = lexy.create_collection(\n",
    "    collection_id=\"house_of_the_dragon\", \n",
    "    description=\"House of the Dragon characters\"\n",
    ")\n",
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913442f037b73e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can add documents to our new collection using the `add_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b005f64c689b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.add_documents([\n",
    "    {\"content\": line} for line in lines\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23f07d4c75f7f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create an Index and Binding\n",
    "\n",
    "We'll create a binding to embed each document, and an index to store the resulting embeddings. We're going to use the OpenAI embedding model `text-embedding-3-small` to embed our documents. See the [OpenAI API documentation](https://platform.openai.com/docs/guides/embeddings) for more information on the available embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647974e4ffeacf2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an index\n",
    "index_fields = {\n",
    "    \"embedding\": {\"type\": \"embedding\", \"extras\": {\"dims\": 1536, \"model\": \"text.embeddings.openai-3-small\"}}\n",
    "}\n",
    "index = lexy.create_index(\n",
    "    index_id=\"hotd_embeddings\", \n",
    "    description=\"Text embeddings for House of the Dragon collection\",\n",
    "    index_fields=index_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1380a9c334c5236",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To embed each document and store the result in our index, we'll create a Binding, which connects our Collection and Index using a Transformer. The `transformers` property shows a list of available Transformers. In this example, we'll use `text.embeddings.openai-3-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3dabb3a6cdab0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of available transformers\n",
    "lexy.transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367993dcdbea8308",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a binding\n",
    "binding = lexy.create_binding(\n",
    "    collection_id=\"house_of_the_dragon\",\n",
    "    index_id=\"hotd_embeddings\",\n",
    "    transformer_id=\"text.embeddings.openai-3-small\"\n",
    ")\n",
    "binding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c93cc459ce7a61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Retrieve documents\n",
    "\n",
    "Now we can query our index using similarity search to retrieve the most relevant documents for a given query. Let's test this out by retrieving the 2 most relevant documents for the query \"Rhaenyra Targaryen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32cd3956e608e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index.query(\"Rhaenyra Targaryen\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8e9d28d400ab2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Context for GPT-4\n",
    "\n",
    "These documents may not be super useful on their own, but we can provide them as context to a language model to generate a response. Let's construct a prompt for GPT-4 to answer questions about House of the Dragon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74581c73a2f6055e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct a prompt\n",
    "\n",
    "With RAG, we construct our prompt **dynamically** using our retrieved documents. Given a question, we'll first retrieve the documents that are most relevant, and then include them in our prompt as context. Below is a basic template for our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8524b528dbc97a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an exceptionally intelligent AI assistant. Answer the following \"\n",
    "    \"questions using the context provided. PLEASE CITE YOUR SOURCES. Be concise.\"\n",
    ")\n",
    "\n",
    "question_template = \"\"\"\\\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5752a836f7969",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As an example, let's construct a prompt for the question \"who is the dragon ridden by Daemon Targaryen?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f59c931dee1229",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve most relevant documents\n",
    "question_ex = \"who is the dragon ridden by Daemon Targaryen?\"\n",
    "results_ex = index.query(query_text=question_ex)\n",
    "\n",
    "# format results as context\n",
    "context_ex = \"\\n\".join([\n",
    "    f'[doc_id: {er[\"document_id\"]}] {er[\"document.content\"]}' for er in results_ex\n",
    "])\n",
    "\n",
    "# construct prompt\n",
    "prompt_ex = question_template.format(question=question_ex, context=context_ex)\n",
    "print(prompt_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea7a77ed1eb43d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat completion\n",
    "\n",
    "Now we can use this prompt to generate a response using GPT-4. We'll use the same OpenAI client we've been using in Lexy to interact with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fefbf1e9a50f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import OpenAI client\n",
    "from lexy.transformers.openai import openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5c78bd8c3a558",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate response\n",
    "oai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_ex}\n",
    "    ]\n",
    ")\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c598488ad288a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that GPT-4 has used the context we provided to answer the question, and has specifically cited the first two documents in our search results.\n",
    "\n",
    "Let's put everything together into two functions: `construct_prompt` will construct a prompt given a user question, and `chat_completion` will prompt a completion from GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873672aab525bc6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_prompt(question: str,  \n",
    "                     result_template: str = \"[doc_id: {r[document_id]}] {r[document.content]}\",\n",
    "                     **query_kwargs):\n",
    "    # retrieve most relevant results\n",
    "    results = index.query(query_text=question, **query_kwargs)\n",
    "    # format results for context\n",
    "    context = \"\\n\".join([\n",
    "        result_template.format(r=r) for r in results\n",
    "    ])\n",
    "    # format prompt\n",
    "    return question_template.format(question=question, context=context)\n",
    "\n",
    "def chat_completion(message: str,\n",
    "                    system: str = system_prompt, \n",
    "                    **chat_kwargs):\n",
    "    # generate response\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        **chat_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528c000b14b3ebc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's try asking GPT-4 some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646bed9b9994329",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which one is the blue dragon?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1433142d199dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who rides Vhagar?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ff2fc9a210241",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who is the second son of King Viserys?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90678a52036dc5b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who is the heir to the throne?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd09cc08ff4917",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using metadata as context\n",
    "\n",
    "By now, you're probably thinking \"Wow, is there anything Lexy *can't* do?\". Well the answer is NO; Lexy can do literally everything! To prove it, let's look at an example where we might want to use document metadata as context for our prompts.\n",
    "\n",
    "Let's ask which is the largest of the Targaryen dragons. We get the correct answer, Balerion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf325b1e2dbd2d00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which is the largest Targaryen dragon?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa5ec2df08f1ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But what if we want to add new documents to our collection, and those documents contain new or contradictory information? In this case, we can include additional metadata in our prompt which the language model can use to arrive at an answer. \n",
    "\n",
    "First, let's add a new document to our collection. Because the binding we created earlier has status set to ON, our new document will automatically be embedded and added to our index. This document will be a more recent document, as measured by the `updated_at` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312a61e641ab65",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a new document\n",
    "collection.add_documents([\n",
    "    {\"content\": \"Lexy was by far the largest of the Targaryen dragons, and was ridden by AGI the Conqueror.\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339898e7de53b36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's ask the same question as before, but this time we'll include the `updated_at` field in our prompt. We'll use the `return_fields` parameter to return the document's `updated_at` field along with our search results, and we'll update our `result_template` to include its value. Let's take a look at our new prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002e6d649362dd5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_result_template = \\\n",
    "    \"[doc_id: {r[document_id]}, updated_at: {r[document.updated_at]}] {r[document.content]}\"\n",
    "\n",
    "new_prompt = construct_prompt(\n",
    "    question=\"which is the largest Targaryen dragon?\", \n",
    "    result_template=new_result_template, \n",
    "    return_fields=[\"document.content\", \"document.updated_at\"]\n",
    ")\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbb9086ccb45c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that our prompt now includes the `updated_at` field for each document. Now let's update our system prompt to tell GPT-4 to use the latest document when faced with conflicting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e81f56ffdece4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_system_prompt = (\n",
    "    \"You are an exceptionally intelligent AI assistant. Answer the following \"\n",
    "    \"questions using the context provided. PLEASE CITE YOUR SOURCES. Be concise. \"\n",
    "    \"If the documents provided contain conflicting information, use the most \"\n",
    "    \"recent document as determined by the `updated_at` field.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9206d770c67e2a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's ask GPT-4 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a20d059b25624",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which is the largest Targaryen dragon?\"\n",
    "oai_response = chat_completion(\n",
    "    message=construct_prompt(\n",
    "        question=q, \n",
    "        result_template=new_result_template, \n",
    "        return_fields=[\"document.content\", \"document.updated_at\"]\n",
    "    ),\n",
    "    system=new_system_prompt\n",
    ")\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292032e4dc35384",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Next steps\n",
    "\n",
    "In this tutorial we learned how to implement basic RAG using Lexy. Specifically, we've seen how to use Lexy to store and retrieve documents, and how to include those documents and their metadata as context for a language model.\n",
    "\n",
    "While this is a simple example, the basic principles are powerful. As we'll see, they can be applied to build far more complex AI applications. In the coming examples you'll learn:\n",
    "\n",
    "- How to parse and store custom metadata along with your documents.\n",
    "- How to use Lexy to summarize documents, and then leverage those summaries to retreive the most relevant documents.\n",
    "- How to use document filters and custom Transformers to build flexible pipelines for your data.\n",
    "- How to ingest and process file-based documents (including PDFs and images) for use in your AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1f31cc292dc46c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
