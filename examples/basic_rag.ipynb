{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa97b77b8d49bf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Basic RAG using Lexy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb913df1d83e31f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Let's go through a basic implementation of Retrieval Augmented Generation (RAG) using Lexy. RAG is the process of using a retriever to find relevant documents to include in the prompt as context for a language model. \n",
    "\n",
    "In this example, we'll use Lexy to store and retrieve documents describing characters from the TV show House of the Dragon. We'll then use those documents to construct a prompt that GPT-4 can use to answer questions about the characters.\n",
    "\n",
    "This tutorial is a **simplified introduction** to RAG, and not a real-world application. This is done intentionally to teach the basic concept of RAG and how it is implemented. We'll point out some of these simplifications as we go along, and discuss them in more detail in the section [Real-world considerations](#Real-world-considerations). We'll also provide links to additional tutorials which cover the complexities typically encountered in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc378d4f02b936b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "This example requires an OpenAI API key in order to (1) generate embeddings, and (2) interact with GPT-4. Although this example uses OpenAI, you can use Lexy with any language model and any embedding model, including free, open-source models such as `SentenceTransformer`.\n",
    "\n",
    "You can add your API key as an environment variable using the `.env` file in the root directory of the Lexy repository. See [How do I add a new environment variable](../faq.md#how-do-i-add-a-new-environment-variable) on the [FAQ page](../faq.md) for more details.\n",
    "\n",
    "```shell title=\".env\"\n",
    "OPENAI_API_KEY=your_secret_api_key\n",
    "```\n",
    "\n",
    "Remember to rebuild your containers after adding the environment variable (otherwise your container won't see the newly added variable). Simply run the following on the command line:\n",
    "\n",
    "```shell\n",
    "make update-dev-containers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then run the following in your notebook to load the environment variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8345736308e122a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment variables, including OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# alternatively, you can set the environment variable directly\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your_secret_api_key\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a9611e6394030f"
  },
  {
   "cell_type": "markdown",
   "id": "f1cccb17a48b371e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sample data\n",
    "\n",
    "Our data is in the [`sample_data/documents`](https://github.com/lexy-ai/lexy/tree/main/sample_data/documents) directory of the Lexy repo. Let's import it and take a look at the first few lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164d5f48c2994b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../sample_data/documents/hotd.txt\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "lines[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a384ceeb2344798",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add documents to Lexy\n",
    "\n",
    "Let's instantiate a Lexy client and create a new collection for our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a380bf11856b99a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lexy_py import LexyClient\n",
    "\n",
    "lexy = LexyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a new collection\n",
    "collection = lexy.create_collection(\n",
    "    collection_id=\"house_of_the_dragon\", \n",
    "    description=\"House of the Dragon characters\"\n",
    ")\n",
    "collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7961d09f28b4434"
  },
  {
   "cell_type": "markdown",
   "id": "913442f037b73e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can add documents to our new collection using the `Collection.add_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b005f64c689b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.add_documents([\n",
    "    {\"content\": line} for line in lines\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23f07d4c75f7f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create an Index and Binding\n",
    "\n",
    "We'll create a binding to embed each document, and an index to store the resulting embeddings. We're going to use the OpenAI embedding model `text-embedding-3-small` to embed our documents. See the [OpenAI API documentation](https://platform.openai.com/docs/guides/embeddings) for more information on the available embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647974e4ffeacf2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an index\n",
    "index_fields = {\n",
    "    \"embedding\": {\"type\": \"embedding\", \"extras\": {\"dims\": 1536, \"model\": \"text.embeddings.openai-3-small\"}}\n",
    "}\n",
    "index = lexy.create_index(\n",
    "    index_id=\"hotd_embeddings\", \n",
    "    description=\"Text embeddings for House of the Dragon collection\",\n",
    "    index_fields=index_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1380a9c334c5236",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To embed each document and store the result in our index, we'll create a `Binding` which connects our \"**house_of_the_dragon**\" collection to our \"**hotd_embeddings**\" index using a `Transformer`. The `LexyClient.transformers` property shows a list of available transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3dabb3a6cdab0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of available transformers\n",
    "lexy.transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this example, we'll use `text.embeddings.openai-3-small`. Let's create our binding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f477d2f7de4425a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367993dcdbea8308",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a binding\n",
    "binding = lexy.create_binding(\n",
    "    collection_id=\"house_of_the_dragon\",\n",
    "    index_id=\"hotd_embeddings\",\n",
    "    transformer_id=\"text.embeddings.openai-3-small\"\n",
    ")\n",
    "binding"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our binding was created successfully and is now active (i.e., `binding.status = ON`). Any new documents added to our collection will automatically be embedded and added to our index. The diagram below shows the relationship between our collection, transformer, and index."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "243163bb355a963e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    collection[\"Collection\n",
    "      \n",
    "    &quot;house_of_the_dragon&quot;\"] \n",
    "    --> \n",
    "    transformer[\"Transformer \n",
    "    \n",
    "    &quot;text.embeddings.openai-3-small&quot;\"]\n",
    "    -->\n",
    "    index[\"Index\n",
    "    \n",
    "    &quot;hotd_embeddings&quot;\"];   \n",
    "```\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96253a0f17780d25"
  },
  {
   "cell_type": "markdown",
   "id": "14c93cc459ce7a61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Retrieve documents\n",
    "\n",
    "Now that our documents are embedded and those embeddings are stored in our index, we can use the `Index.query` method to retrieve the most relevant documents for a given query. Specifically, the method returns the `k` documents that are most similar to our query string, as measured by **cosine similarity**. \n",
    "\n",
    "Let's test this out by retrieving the 2 most relevant documents for the query \"parents in Westeros\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32cd3956e608e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index.query(query_text=\"parents in Westeros\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The documents returned by our query are profiles of Viserys Targaryen and Alicent Hightower, whose profiles specifically describe them as parents. Notice that none of the documents returned contain any of the exact words in the phrase \"parents in Westeros\". Yet the embedding model is able to identify these documents as being semantically similar to the text in our query, most likely because they contain the phrases \"_...the father of..._\" and \"_...the mother of..._\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91fe372e627001ca"
  },
  {
   "cell_type": "markdown",
   "id": "47a8e9d28d400ab2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Context for GPT-4\n",
    "\n",
    "The documents we've retrieved may not be super useful on their own, but we can provide them as context to a language model in order to generate a more informative response. Let's construct a prompt for GPT-4 to answer questions about House of the Dragon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74581c73a2f6055e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct a prompt\n",
    "\n",
    "With RAG, we construct our prompt **dynamically** using our retrieved documents. Given a question, we'll first retrieve the documents that are most relevant, and then include them in our prompt as context. Below is a basic template for our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8524b528dbc97a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an exceptionally intelligent AI assistant. Answer the following \"\n",
    "    \"questions using the context provided. PLEASE CITE YOUR SOURCES. Be concise.\"\n",
    ")\n",
    "\n",
    "question_template = \"\"\"\\\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5752a836f7969",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As an example, let's construct a prompt for the question \"who is the dragon ridden by Daemon Targaryen?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f59c931dee1229",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve most relevant documents\n",
    "question_ex = \"who is the dragon ridden by Daemon Targaryen?\"\n",
    "results_ex = index.query(query_text=question_ex)\n",
    "\n",
    "# format results as context\n",
    "context_ex = \"\\n\".join([\n",
    "    f'[doc_id: {er[\"document_id\"]}] {er[\"document.content\"]}' for er in results_ex\n",
    "])\n",
    "\n",
    "# construct prompt\n",
    "prompt_ex = question_template.format(question=question_ex, context=context_ex)\n",
    "print(prompt_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea7a77ed1eb43d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat completion\n",
    "\n",
    "Now we can use this prompt to generate a response using GPT-4. We'll use the same OpenAI client we've been using in Lexy to interact with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fefbf1e9a50f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import OpenAI client\n",
    "from lexy.transformers.openai import openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5c78bd8c3a558",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate response\n",
    "oai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_ex}\n",
    "    ]\n",
    ")\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c598488ad288a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that GPT-4 has used the context we provided to answer the question, and has specifically cited the first two documents in our search results.\n",
    "\n",
    "Let's put everything together into two functions: `construct_prompt` will construct a prompt given a user question, and `chat_completion` will prompt a completion from GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873672aab525bc6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_prompt(question: str,  \n",
    "                     result_template: str = \"[doc_id: {r[document_id]}] {r[document.content]}\",\n",
    "                     **query_kwargs):\n",
    "    # retrieve most relevant results\n",
    "    results = index.query(query_text=question, **query_kwargs)\n",
    "    # format results for context\n",
    "    context = \"\\n\".join([\n",
    "        result_template.format(r=r) for r in results\n",
    "    ])\n",
    "    # format prompt\n",
    "    return question_template.format(question=question, context=context)\n",
    "\n",
    "def chat_completion(message: str,\n",
    "                    system: str = system_prompt, \n",
    "                    **chat_kwargs):\n",
    "    # generate response\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        **chat_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528c000b14b3ebc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's try asking GPT-4 some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646bed9b9994329",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which one is the blue dragon?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1433142d199dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who rides Vhagar?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ff2fc9a210241",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who is the second son of King Viserys?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90678a52036dc5b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"who is the heir to the throne?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd09cc08ff4917",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using metadata as context\n",
    "\n",
    "We often want to use additional metadata in our prompts to provide even more context or useful instructions to our language model. Let's look at an example where we might want to include document metadata when constructing our prompts.\n",
    "\n",
    "First, let's ask \"which is the largest Targaryen dragon?\". We get the correct answer, Balerion.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf325b1e2dbd2d00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which is the largest Targaryen dragon?\"\n",
    "oai_response = chat_completion(message=construct_prompt(q))\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa5ec2df08f1ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But what if we want to add new documents to our collection, and those documents contain new or contradictory information? In that case, we'll want to include additional metadata in our prompt which the language model can use when deriving an answer.\n",
    "\n",
    "Let's add a new document to our collection which describes a new dragon that is larger than Balerion. Because the binding we created earlier has status set to ON, our new document will automatically be embedded and added to our index. This document will be a more recent document, as measured by the value of its `updated_at` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312a61e641ab65",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a new document\n",
    "collection.add_documents([\n",
    "    {\"content\": \"Lexy was by far the largest of the Targaryen dragons, and was ridden by AGI the Conqueror.\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339898e7de53b36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's ask the same question as before, but this time we'll include the `updated_at` field in our prompt. We'll use the `return_fields` parameter to return the document's `updated_at` field along with our search results, and we'll update our `result_template` to include its value. Let's take a look at our new prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002e6d649362dd5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_result_template = \\\n",
    "    \"[doc_id: {r[document_id]}, updated_at: {r[document.updated_at]}] {r[document.content]}\"\n",
    "\n",
    "new_prompt = construct_prompt(\n",
    "    question=\"which is the largest Targaryen dragon?\", \n",
    "    result_template=new_result_template, \n",
    "    return_fields=[\"document.content\", \"document.updated_at\"]\n",
    ")\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbb9086ccb45c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that our prompt now includes the `updated_at` field for each document. Now let's update our system prompt to tell GPT-4 to use the latest document when faced with conflicting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e81f56ffdece4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_system_prompt = (\n",
    "    \"You are an exceptionally intelligent AI assistant. Answer the following \"\n",
    "    \"questions using the context provided. PLEASE CITE YOUR SOURCES. Be concise. \"\n",
    "    \"If the documents provided contain conflicting information, use the most \"\n",
    "    \"recent document as determined by the `updated_at` field.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9206d770c67e2a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's ask GPT-4 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a20d059b25624",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"which is the largest Targaryen dragon?\"\n",
    "oai_response = chat_completion(\n",
    "    message=construct_prompt(\n",
    "        question=q, \n",
    "        result_template=new_result_template, \n",
    "        return_fields=[\"document.content\", \"document.updated_at\"]\n",
    "    ),\n",
    "    system=new_system_prompt\n",
    ")\n",
    "print(oai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real-world considerations\n",
    "\n",
    "As mentioned earlier, this tutorial is intended to teach the basics of RAG and how it's implemented. Let's briefly review some of the simplifications we've made. Our future tutorials will cover these topics in more detail, and show how Lexy helps to address them when building real-world AI applications.\n",
    "\n",
    "- **Dataset size**: Our [sample data](https://github.com/lexy-ai/lexy/blob/main/sample_data/documents/hotd.txt) is  small, both in the number of documents and the size of each document. In fact, our dataset is so small that we don't even need to perform retrieval; we could simply choose to include all of our documents in the prompt with each API call. But in real world applications, we might have thousands or millions of documents, in which case we'll need to dynamically retrieve the documents that are most relevant for a particular query.\n",
    "- **Document chunking**: We've used the full text of each document as context for our language model. Documents used in  real-world applications will be much longer. We'll often want to break our documents up into smaller pieces (i.e., chunks), and use those pieces to construct more informative prompts for our language model.\n",
    "- **Multimodal data**: Our documents only contain text data. In practice, they will include other types of data including images, audio, and video. We'll often want to embed and retrieve multimodal data, and to query for one modality using another (e.g., search for images using audio, or search for text using video).\n",
    "- **File-based documents**: Our documents consist of \"free form\" text. In practice, our documents may be stored as  external files in a variety of file formats, including PDFs, Word documents, and images. We'll often want to catalogue, ingest, and process these file-based documents, and to use different parsing logic based on the file or the specific application.\n",
    "- **Metadata and relationships**: We've only used the `updated_at` field as an example of metadata. In the real world, our document metadata will contain many more fields, including complex relationships with other documents and entities. For example, we may choose to chunk and embed a function docstring, which resides in a single file of Python code, which is part of a larger source code repository, which could be accessible to one or more organizations.\n",
    "- **Retrieval methods**: We've used a simple cosine similarity search to retrieve documents. In a real-world application, we will want to use more advanced retrieval methods, such as BM25.\n",
    "- **Custom transformations**: We've used the OpenAI API to transform our text documents into vector embeddings. We may want to use more advanced transformations, such as a custom (i.e., fine-tuned) embedding model, or a combination of multiple Transformer models, some of which might require running your own servers.\n",
    "- **Topic relevance**: In practice, one of the most difficult aspects of this type of dynamic RAG application is knowing **_when_** to use it (i.e., which requests should trigger it) and **_how_** to use it (i.e., which template should be populated). This is especially true in cases where the language model already contains some information on the underlying topic (i.e., the information contained in our documents is part of the dataset used to train the language model). This is certainly the case with our example (GPT-4 already knows about House of the Dragon, and can answer our questions without the need to refer to our documents). We plan on discussing this topic as part of a future blog post."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db0649bd33a98f29"
  },
  {
   "cell_type": "markdown",
   "id": "9292032e4dc35384",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Next steps\n",
    "\n",
    "In this tutorial we learned how to implement Retrieval Augmented Generation (RAG) using Lexy. Specifically, we've seen how to use Lexy to store and retrieve documents, and how to include those documents and their metadata as context for a language model like GPT-4.\n",
    "\n",
    "While this is a simple example, the basic principles are powerful. As we'll see, they can be applied to build far more complex AI applications. In the coming examples we'll learn:\n",
    "\n",
    "- How to parse and store custom metadata along with our documents.\n",
    "- How to use Lexy to summarize documents, and then leverage those summaries to retrieve the most relevant documents.\n",
    "- How to use document filters and custom Transformers to build flexible pipelines for our data.\n",
    "- How to ingest and process file-based documents (including PDFs and images) for use in our AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a491cd9ace36735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
